{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zack1243/Novel-Webscraper/blob/multi-threaded-downloads/Webscraper3_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsn3bcPnF-rS",
        "outputId": "0b04b32a-55b6-44ed-a27d-49c39422372d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: selenium in /usr/local/lib/python3.10/dist-packages (4.20.0)\n",
            "Requirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (2.0.7)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.25.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.11.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.2.2)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.11.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.7)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.10/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup as b4\n",
        "import asyncio\n",
        "from google.colab import files\n",
        "import multiprocessing\n",
        "import os\n",
        "import requests\n",
        "import time\n",
        "import zipfile\n",
        "!pip install selenium"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import selenium\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import TimeoutException"
      ],
      "metadata": {
        "id": "3ckpW6a_36_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "En4RRes9BYNT"
      },
      "outputs": [],
      "source": [
        "TRIAL = 3\n",
        "NUM_THREADS = 5\n",
        "LINKS = [\"https://www.lightnovelpub.com/novel/advent-of-the-three-calamities-1678/chapter-1\",\n",
        "  \"https://www.lightnovelpub.com/novel/advent-of-the-three-calamities-1678/chapter-2\",\n",
        "  \"https://www.lightnovelpub.com/novel/advent-of-the-three-calamities-1678/chapter-3\",\n",
        "  \"https://www.lightnovelpub.com/novel/advent-of-the-three-calamities-1678/chapter-4\",\n",
        "  \"https://www.lightnovelpub.com/novel/advent-of-the-three-calamities-1678/chapter-5\",\n",
        "  \"https://www.lightnovelpub.com/novel/advent-of-the-three-calamities-1678/chapter-6\",\n",
        "  \"https://www.lightnovelpub.com/novel/advent-of-the-three-calamities-1678/chapter-7\",\n",
        "  \"https://www.lightnovelpub.com/novel/advent-of-the-three-calamities-1678/chapter-8\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instantiate browser"
      ],
      "metadata": {
        "id": "vg8phXhR6eP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getBrowser():\n",
        "  options = webdriver.FirefoxOptions()\n",
        "  options.add_argument('--headless')\n",
        "  browser = webdriver.Firefox(options=options)\n",
        "  return browser"
      ],
      "metadata": {
        "id": "fGUf7k1_6dQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIfWjf7DEU4a"
      },
      "source": [
        "### Parse chapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICR4B894EUlm"
      },
      "outputs": [],
      "source": [
        "async def parseFile(html):\n",
        "\n",
        "  # Start beautiful soup object and parse\n",
        "  soup = b4(html, features=\"html.parser\")\n",
        "  tag = soup.article.find_all('span')\n",
        "\n",
        "  # Get title\n",
        "  for t in tag:\n",
        "    if \"Chapter\" in t.get_text():\n",
        "      title = t.get_text() + \".html\"\n",
        "\n",
        "  # Get chapter content\n",
        "  soup = soup.find_all('p')\n",
        "  with open(title, \"w\" , encoding='utf-8') as file:\n",
        "    for paragraph in soup:\n",
        "        file.write(str(paragraph) + \"\\n\")  # Write each paragraph to the file\n",
        "    print(str(soup))\n",
        "  file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sIu5yOw8uMi"
      },
      "source": [
        "### Download main webpage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04v_pqZ80XCk"
      },
      "outputs": [],
      "source": [
        "async def getChapter(queue, browser, url, trial):\n",
        "  # while True:\n",
        "  try:\n",
        "    browser.get(url)\n",
        "    print(f\"attempting to download url: {url}\")\n",
        "    timeout_in_seconds = 10\n",
        "    WebDriverWait(browser, timeout_in_seconds).until(EC.presence_of_element_located((By.ID, 'chapter-container')))\n",
        "    await parseFile(browser.page_source)\n",
        "  except:\n",
        "    if trial >= TRIAL:\n",
        "      with open(url, \"w\") as file:\n",
        "        file.write(\"I DONE GOOFED!!!\")\n",
        "      file.close()\n",
        "      print(\"I give up...\")\n",
        "      queue.task_done()\n",
        "    else:\n",
        "      print(f\"Download failed trial {trial}\")\n",
        "      trial += 1\n",
        "      await getChapter(queue, browser, url, trial)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Chapter page links"
      ],
      "metadata": {
        "id": "W4WMhfNg_Z4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def getChapterPage(queue, browser, url, trial):\n",
        "  try:\n",
        "    # Get webpage\n",
        "    print(f\"getChapterPages trying url: {url}...\")\n",
        "    browser.get(url)\n",
        "\n",
        "    # Start beautiful soup html parsing\n",
        "    soup = b4(browser.page_source, features=\"html.parser\")\n",
        "    links = soup.find(class_=\"pagenav\").find_all(class_=\"PagedList-skipToNext\")\n",
        "\n",
        "    # Append chapter and chapter page links\n",
        "    if links:\n",
        "      queue.put_nowait(\"https://www.lightnovelpub.com\" + links[0].find('a')['href'])\n",
        "    strainer = soup.find(class_=\"chapter-list\").find_all('a')\n",
        "    for link in strainer:\n",
        "      queue.put_nowait(\"https://www.lightnovelpub.com\"+ link['href'])\n",
        "\n",
        "\n",
        "  # Try to get webpage a total of TRIAL times before giving up\n",
        "  except TimeoutException:\n",
        "    if trial >= TRIAL:\n",
        "      with open(url, \"w\") as file:\n",
        "        file.write(\"I DONE GOOFED!!!\")\n",
        "      file.close()\n",
        "      print(\"I give up...\")\n",
        "    else:\n",
        "      print(f\"Download failed trial {trial}\")\n",
        "      getChapterPage(queue, browser, url, trial + 1)"
      ],
      "metadata": {
        "id": "cVYePu_O_Zbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76fZKCAe03o9"
      },
      "source": [
        "### Get links and go to next chapter page, if avaliable (testing from file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s54J3idj03Uv"
      },
      "outputs": [],
      "source": [
        "async def getChapterPages(queue, browser):\n",
        "\n",
        "  # Fet item from queue\n",
        "  url = await queue.get()\n",
        "\n",
        "  # If this is a chapter page\n",
        "  if \"page\" in url:\n",
        "    getChapterPage(queue, browser, url, 0)\n",
        "\n",
        "  # Else, this url is a chapter\n",
        "  else:\n",
        "    getChapter(queue, browser, url, 0)\n",
        "\n",
        "  # Mark as done\n",
        "  queue.task_done()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the first webpage"
      ],
      "metadata": {
        "id": "Q0h0Car9SduK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO\n",
        "  # Add the link to list of failed downloads if failed\n",
        "def getFirstPage(url, browser, trial):\n",
        "  try:\n",
        "    queue = asyncio.Queue()\n",
        "\n",
        "    # Get and parse webpage\n",
        "    browser.get(url)\n",
        "    soup = b4(browser.page_source, features=\"html.parser\")\n",
        "\n",
        "    # Get chapter links and chapter page links\n",
        "    #TODO: Error check if the next page does not exist\n",
        "    links = soup.find(class_=\"pagenav\").find_all(class_=\"PagedList-skipToNext\")\n",
        "    strainer = soup.find(class_=\"chapter-list\").find_all('a')\n",
        "\n",
        "    # Found both chapter page links and chapter links\n",
        "    if links:\n",
        "      # Queue the next chapter page url\n",
        "      queue.append(\"https://www.lightnovelpub.com\" + links[0].find('a')['href'])\n",
        "      for link in strainer:\n",
        "        queue.append(\"https://www.lightnovelpub.com\"+ link['href'])\n",
        "      return queue\n",
        "\n",
        "    # Could only find chapter links\n",
        "    elif strainer:\n",
        "      for link in strainer:\n",
        "        queue.append(\"https://www.lightnovelpub.com\"+ link['href'])\n",
        "      return queue\n",
        "\n",
        "    # Got nothing\n",
        "    else:\n",
        "      return -1\n",
        "\n",
        "  # Failed to download webpage\n",
        "  except:\n",
        "    # Give up!\n",
        "    if trial >= TRIAL:\n",
        "      # Write a file saying what failed\n",
        "      print(f\"Failed to find/get input url: {url}\")\n",
        "      return -1\n",
        "\n",
        "    # Try again, up to 3 times\n",
        "    else:\n",
        "      print(f\"Download failed trial {trial}\")\n",
        "      getFirstPage(url, browser, trial + 1)"
      ],
      "metadata": {
        "id": "pdMfZ-P-Sccy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Async Download Chapter pages"
      ],
      "metadata": {
        "id": "ZVU8cWb8_8WX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def asyncDownload(url, browser):\n",
        "  queue = getFirstPage(url, browser, 0)\n",
        "\n",
        "  if queue == -1:\n",
        "    return\n",
        "\n",
        "  # Adjust threads to queue size\n",
        "  if queue.qsize() < NUM_THREADS:\n",
        "    NUM_THREADS = queue.qsize()\n",
        "\n",
        "  # Instantiate task list and browsers\n",
        "  tasks = []\n",
        "  browsers = []\n",
        "  for i in range(NUM_THREADS):\n",
        "    browsers.append(getBrowser())\n",
        "    task = asyncio.create_task(getChapterPages(queue, browsers[i]))\n",
        "    tasks.append(task)\n",
        "\n",
        "  # Wait for all queue items to be downloaded\n",
        "  queue.join()\n",
        "\n",
        "  # Cancel all tasks\n",
        "  for task in tasks:\n",
        "    task.cancel()\n",
        "\n",
        "  # Wait for tasks to be canceled\n",
        "  await asyncio.gather(*tasks, return_exceptions=True)\n",
        "\n",
        "  # Quit all browsers\n",
        "  for browser in browsers:\n",
        "    browser.quit()\n",
        "\n",
        "\n",
        "  # Error checking\n",
        "    # Check at the last chapter page\n",
        "    # Check if there is a next chapter page\n",
        "    # List of failed downloads"
      ],
      "metadata": {
        "id": "SYheEB7p_79-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zip up files to be downloaded"
      ],
      "metadata": {
        "id": "vxilu1N5-DbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def downloadZip(title):\n",
        "  directory = os.getcwd()\n",
        "\n",
        "  # List all files in the directory\n",
        "  all_files = os.listdir(directory)\n",
        "\n",
        "  # Specify the name of the zip file to create\n",
        "  zip_file_name = title + \".zip\"\n",
        "\n",
        "  # Create a zip file\n",
        "  with zipfile.ZipFile(zip_file_name, 'w') as zipf:\n",
        "      # Add each file in the directory to the zip file\n",
        "      for file in all_files:\n",
        "          file_path = os.path.join(directory, file)\n",
        "          zipf.write(file_path, os.path.basename(file_path))\n",
        "\n",
        "  print(\"Zip file created:\", zip_file_name)\n",
        "\n",
        "  files.download(directory + '/' + zip_file_name)"
      ],
      "metadata": {
        "id": "VqycNJAi-C_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make and change to directory (book's title)"
      ],
      "metadata": {
        "id": "LbR20WyL-xqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getDir(directory):\n",
        "  if not os.path.exists(directory):\n",
        "    os.mkdir(directory)\n",
        "    os.chdir(directory)\n",
        "  elif os.getcwd != directory:\n",
        "    os.chdir(directory)\n",
        "  else:\n",
        "    pass"
      ],
      "metadata": {
        "id": "BGtO7bt6-Q6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  def main1(url, title):\n",
        "    # Change directory\n",
        "    getDir(title)\n",
        "\n",
        "    # Start async downloading chapter pages\n",
        "    asyncDownload(url, getBrowser())\n",
        "\n",
        "    # Zip and download chapters\n",
        "    downloadZip(title)"
      ],
      "metadata": {
        "id": "QRbJCTWt_UCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pvQyXi8HZYK"
      },
      "source": [
        "### Input url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xi9DH-juGBLV"
      },
      "outputs": [],
      "source": [
        "# Ask user for ln information\n",
        "url = input(\"Enter URL: \")\n",
        "title = input(\"Enter directory: \")\n",
        "main1(url, title)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhlT7q1RAH8+e2euqFmulV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}