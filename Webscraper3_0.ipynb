{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHHXV6yJCphYYEmWHDesRF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zack1243/Novel-Webscraper/blob/main/Webscraper3_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsn3bcPnF-rS",
        "outputId": "0ec28278-b89c-46c3-eba5-aa83d3b538ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: selenium in /usr/local/lib/python3.10/dist-packages (4.20.0)\n",
            "Requirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (2.0.7)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.25.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.11.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.2.2)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.11.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.7)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.10/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup as b4\n",
        "import asyncio\n",
        "import multiprocessing\n",
        "import time\n",
        "import requests\n",
        "!pip install selenium\n",
        "import selenium\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import TimeoutException"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRIAL = 3"
      ],
      "metadata": {
        "id": "En4RRes9BYNT"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parse chapter"
      ],
      "metadata": {
        "id": "aIfWjf7DEU4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parseFile(html):\n",
        "  #with open(html, \"r\") as file:\n",
        "  soup = b4(html, features=\"html.parser\")\n",
        "  #print(soup.chapter-title.get_text())\n",
        "  tag = soup.article.find_all('span')\n",
        "  content = \"\"\n",
        "  for t in tag:\n",
        "    if \"Chapter\" in t.get_text():\n",
        "      title = t.get_text() + \".html\"\n",
        "  soup = soup.find_all('p')\n",
        "  with open(title, \"w\" , encoding='utf-8') as file:\n",
        "    for paragraph in soup:\n",
        "        file.write(str(paragraph) + \"\\n\")  # Write each paragraph to the file\n",
        "    print(str(soup))\n",
        "  file.close()"
      ],
      "metadata": {
        "id": "ICR4B894EUlm"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download main webpage"
      ],
      "metadata": {
        "id": "8sIu5yOw8uMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getChapter(url, browser, trial):\n",
        "  try:\n",
        "      browser.get(url)\n",
        "      timeout_in_seconds = 10\n",
        "      WebDriverWait(browser, timeout_in_seconds).until(EC.presence_of_element_located((By.ID, 'chapter-container')))\n",
        "      html = browser.page_source\n",
        "      parseFile(html)\n",
        "  except TimeoutException:\n",
        "      if trial >= TRIAL:\n",
        "        with open(url, \"w\") as file:\n",
        "          file.write(\"I DONE GOOFED!!!\")\n",
        "        file.close()\n",
        "        print(\"I give up...\")\n",
        "      else:\n",
        "        getChapter(url, browser, trial + 1)\n",
        "  finally:\n",
        "      browser.quit()"
      ],
      "metadata": {
        "id": "04v_pqZ80XCk"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get webpage"
      ],
      "metadata": {
        "id": "voWZeQopEqKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getNovelPage(url, browser, trial):\n",
        "  try:\n",
        "      browser.get(url)\n",
        "      html = browser.page_source\n",
        "      soup = b4(html, features=\"html.parser\")\n",
        "      strainer = soup.find(class_=\"chapter-list\").find_all('a')\n",
        "      for link in strainer:\n",
        "        new_url = url.replace(\"/chapters\", \"\") + link['href']\n",
        "        getChapter(new_url, browser, 0)\n",
        "      return html\n",
        "  except TimeoutException:\n",
        "      if trial >= TRIAL:\n",
        "        with open(url, \"w\") as file:\n",
        "          file.write(\"I DONE GOOFED!!!\")\n",
        "        return -1\n",
        "        file.close()\n",
        "        print(\"I give up...\")\n",
        "      else:\n",
        "        getNovelPage(url, browser, trial + 1)\n",
        "  finally:\n",
        "      browser.quit()"
      ],
      "metadata": {
        "id": "kjYHeIIqEppo"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get links and go to next chapter page, if avaliable (testing from file)"
      ],
      "metadata": {
        "id": "76fZKCAe03o9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getChapterPages(url, page_num):\n",
        "  # Start a new selenium process\n",
        "  options = webdriver.FirefoxOptions()\n",
        "  options.add_argument('--headless')\n",
        "  browser = webdriver.Firefox(options=options)\n",
        "\n",
        "  # Get the novel main page\n",
        "  url_source = getNovelPage(url, browser, 0)\n",
        "\n",
        "  if url_source == -1:\n",
        "    print(\"Unable to find page source!\")\n",
        "    return\n",
        "\n",
        "  soup = b4(url_source, features=\"html.parser\")\n",
        "  links = soup.find(class_=\"pagenav\").find_all(class_=\"PagedList-skipToNext\")\n",
        "  next_url = \"\"\n",
        "  for link in links:\n",
        "    next_url = link.find('a')['href']\n",
        "\n",
        "  # Make new url\n",
        "  new_url = url.replace(\"/chapters\", \"\") + next_url\n",
        "  getChapterPages(new_url, page_num+1)"
      ],
      "metadata": {
        "id": "s54J3idj03Uv"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input url"
      ],
      "metadata": {
        "id": "4pvQyXi8HZYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = input(\"Enter URL: \")\n",
        "#sample url: https://www.lightnovelpub.com/novel/chrysalis-16091353/chapters\n",
        "getChapterPages(url, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xi9DH-juGBLV",
        "outputId": "30b2b6b3-9441-4ef3-e4f7-dbb31a78e6d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter URL: https://www.lightnovelpub.com/novel/chrysalis-16091353/chapters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "fGxkkfYJIFv0"
      }
    }
  ]
}